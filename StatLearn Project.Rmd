---
title: "Stat Learn Project"
author: "Lillian Jarrett, Samuel Brown, Stephen Kullman"
output: html_document
date: "2023-11-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(scales)
# install.packages("ROSE")
library(ROSE)
library(caTools)
library(caret)
# install.packages("PRROC")
library(PRROC)
```

## R Markdown
Load in the data...
```{r}
data <- read.csv("/Users/stephenkullman/Desktop/School of Data Science/STAT 6030/HW/creditcard_data.csv")
```


```{r}
head(data)
```
Any NA values.....
```{r}
sum(is.null(data))
```

How balanced is the data?
```{r}
data %>% 
  group_by(Class) %>% 
  tally()
```

Not very balanced... That will be a challenge when trying to train a model.

Plot of Amount vs Time
```{r}
ggplot(data = data, aes(x=Time,y=Amount))+
  geom_point()+
  facet_grid(rows = vars(Class))

```



```{r}
data %>% 
  group_by(Class) %>% 
  summarize(avg_amt = mean(Amount))
```
Correlations
```{r}
corr_map <- round(cor(data),2)
corr_map
```
Class changed to a factor
```{r}
data$Class <- as.factor(data$Class)
str(data$Class)
```

Scaling time and amount
```{r}
scaled <- data
scaled$Time <- scale(scaled$Time)
scaled$Amount <- scale(scaled$Amount)
```

Train/Test Split
```{r}
set.seed(1738)
ind = sample.split(scaled, SplitRatio = 0.8) # split ratio is 0.8.
train = scaled[ind, ]
test =  scaled[!ind, ]
```

Under sampling to create a balanced training set
```{r}
nrow_fraud <- nrow(train[train$Class == 1, ])

undersample_frac <- 0.2

# find the desired sample size when applying the undersampling technique.
undersample_size <- nrow_fraud/undersample_frac

# undersample the dataset
undersample <- ovun.sample(Class ~ ., data = train, method = "under", N = undersample_size, seed = 1738)

undersample_df <- undersample$data

print("Class Frequency before and after using undersampling techniques || 0: Not Fraud & 1: Fraud")
cbind(Before = table(train$Class), After = table(undersample_df$Class))
```


```{r}
# create list of models 
model_name <- c("knn")

# this variable is used to store the evaluation metrics
evaluation <- matrix(nrow = 0, ncol = 5) 
# this variable is used to store confusion matrices
list_confusion_matrix <- list() 

for (methods in model_name){
    # build model on train set
    set.seed(1738)
    model <- train(Class ~ ., 
                   data = undersample_df, 
                   method = methods, 
                   trControl = trainControl(method = "cv", number = 5))
    # print the model results 
    print(model)

    # use the model to predict on test set
    predictions <- predict(model, newdata = test) # predict the target
    preds <- predict(model, newdata = test, type = "prob")[,2] # predict probability of positive class
    preds_pos <- preds[test$Class==1] # preds for true positive class
    preds_neg <- preds[test$Class==0] # preds for true negative class

    # calculate evaluation metrics 
    accuracy <- sum(predictions == test$Class) / length(test$Class)
    cm <- confusionMatrix(predictions, test$Class)$table
    precision <- cm[2, 2]/(cm[2, 2] + cm[2, 1]) # TP/(TP+FP)
    recall <- cm[2, 2]/(cm[2, 2] + cm[1, 2]) # TP/(TP+FN)
    f1 <- 2 * (precision * recall) / (precision + recall)
    auprc <- pr.curve(preds_pos, preds_neg)$auc.integral

    # store calculated metrics and confusion matrices
    evaluation <- rbind(evaluation, c(accuracy, precision, recall, f1, auprc))
    list_confusion_matrix[[length(list_confusion_matrix)+1]] <- list(cm)
}
```



```{r}
undersample_copy <- undersample_df
undersample_copy$Class <- ifelse(undersample_copy$Class == 1, "Yes", "No")

hyper_grid <- expand.grid(
  # k = floor(seq(1, nrow(undersample_copy)/3, length.out = 20))
  k = floor(seq(1, 247, by=2))
)

knn_grid <- train(
  Class~.,
  data = undersample_copy, 
  method = "knn", 
  trControl = trainControl(method = "cv", number = 5, classProbs = TRUE),  
  tuneGrid = hyper_grid,
  metric = "ROC"
)

ggplot(knn_grid)

```
















